# CodeSense

## What is CodeSense?

## Why CodeSense?

## Features

## How It Works (Architecture)

## Installation

## Usage

## Initialization (Important)

## Commands

## Tech Stack

## Security & Privacy

## Limitations

## Future Improvements

# ðŸš€ Project Documentation

Generated by codesense !!

### ðŸš€ Project Documentation

Generated by codesense !!

### Project Overview

CodeSense is a local code intelligence CLI application designed to assist backend engineers by providing natural language querying capabilities over their codebase. It leverages Retrieval-Augmented Generation (RAG) principles to scan, chunk, embed, and index a project's files into a local vector database. This allows users to ask questions about their code, generate context-aware READMEs, and manage the indexing state of their repositories directly from the terminal.

The primary function of CodeSense is to enable deep understanding and documentation of a codebase without requiring external services for the core retrieval process, only for embeddings and Large Language Model (LLM) interactions. It aims to make code exploration more efficient by providing concise, technically deep answers and structured output in the command line.

### Core Architecture

The CodeSense CLI follows a modular architecture, separating concerns into commands, core ingestion logic, RAG components, LLM interaction, and utility functions.

- **CLI Entry Point (`codesense.js`)**: This file serves as the main executable. It parses command-line arguments, ensures the necessary environment variables are set, checks if the repository is indexed (unless running `init` or `index`), and dispatches to the appropriate command handler based on user input.
- **Commands (`commands/*.js`)**: Each user-facing command (e.g., `init`, `index`, `ask`, `readme`, `reset`, `status`) is implemented as a separate module, invoked by the main CLI entry point.
- **Ingestion (`core/ingest/*.js`)**: This module group handles the processing of source code for indexing.
  - `scanRepo.js`: Recursively scans a directory, excluding specified ignored files and folders.
  - `filter.js`: Filters scanned files based on allowed/blocked extensions and assigns a `role` (e.g., `config`, `entry`, `route`) to each file based on predefined rules and path patterns.
  - `chunker.js`: Reads the content of filtered files and bundles them into "chunks" containing file path, role, and content.
  - `indexState.js`: Manages the `status.json` file which tracks whether a repository has been indexed.
- **RAG Components (`core/rag/*.js`)**: These modules facilitate the Retrieval-Augmented Generation workflow.
  - `vector.js`: Manages the local vector database (`vectra`). It handles embedding text using the Google Gemini API (`text-embedding-004`), storing text and its metadata, and performing vector similarity searches. It also provides `hardResetDatabase` functionality.
  - `retrieve.js`: A wrapper around `vector.js`'s `searchVectors` to simplify the retrieval of context chunks based on a query.
  - `getGlobalContext.js`: Gathers a broad context of the codebase by querying for key architectural topics, useful for tasks like README generation.
- **LLM Interaction (`core/llm/*.js`)**:
  - `generate.js`: Interfaces with the Google Generative AI API (`gemini-2.5-flash`) to generate answers. It constructs a detailed prompt using contextual data and task descriptions, and includes error handling for API issues and safety filters.
  - `readmeTasks.js`: Defines various prompts used specifically for README generation.
- **Utilities (`core/utils/*.js`)**:
  - `ensureEnv.js`: Verifies the presence of the `GEMINI_API_KEY` environment variable.
  - `getStorePaths.js`: Provides standardized paths for the `.codesense` directory and its contents (`vectordb`, `status.json`).

### Primary Logic

#### Indexing Workflow

The `index` command (handled by `commands/index.js`, which wraps `indexRepository` from `core/ingest/index.js`) executes the following steps:

1.  **Scan**: `scanRepo(repoPath)` recursively walks the file system, collecting all non-ignored file paths.
2.  **Filter**: `filter(scannedFiles)` processes these paths, excluding files with `blockedExts` (e.g., `.css`, `.md`) and including only `allowedExts` (e.g., `.js`, `.json`). During this step, a `role` (e.g., `config`, `entry`) is assigned to each file based on its name or path.
3.  **Chunk**: `chunkFile(filteredFiles)` reads the content of each filtered file, creating data objects (`{ id, role, file, content }`).
4.  **Embed and Store**: For each chunk, `storeText(chunk.content, chunk.metadata)` is called. This function:
    - Ensures the `vectra` index is created.
    - Generates a vector embedding of the chunk's `content` using `getEmbedding` (which calls the Gemini `text-embedding-004` model).
    - Inserts the vector and its metadata (including file, role, and the original text) into the `LocalIndex` provided by `vectra`.
5.  **State Management**: Upon successful indexing, `writeIndexState()` records the indexing status and timestamp.

#### Asking (Retrieval-Augmented Generation) Workflow

The `ask` command (`commands/ask.js`) performs the following:

1.  **Retrieve Context**: `retrieve(question, 4)` is called. This function:
    - Uses `searchVectors(query, limit)` from `core/rag/vector.js`.
    - `searchVectors` takes the user's `question`, generates an embedding for it, and then queries the local vector database to find the `limit` (default 4) most semantically similar code chunks.
    - The `metadata` (including file, role, and text content) of these relevant chunks are returned.
2.  **Generate Answer**: `generateAnswer(question, contextChunks)` is invoked. This function:
    - Constructs a comprehensive prompt for the LLM, incorporating the `question` (task) and the `contextChunks` (contextual data).
    - The prompt specifies a "senior backend engineer" role, strict terminal formatting, and a mandate to use _only_ the provided context.
    - It sends this prompt to the Google Gemini `gemini-2.5-flash` model.
    - Handles potential API errors (e.g., quota, auth, network) and LLM-specific issues (safety filters, empty responses).
3.  **Display Output**: The generated answer, which is expected to be in Markdown format, is then rendered to the terminal using `marked` and `marked-terminal` for a visually appealing output.

#### Environment Setup (`initCommand`)

The `initCommand` in `commands/init.js` guides the user through setting up their `GEMINI_API_KEY`.

1.  It checks for an existing `.env` file and `GEMINI_API_KEY`.
2.  If the key exists, it prompts the user to confirm if they wish to update it.
3.  It securely prompts the user for their Gemini API Key using `askSecret` (which hides input).
4.  The provided key is then written or updated in the `.env` file in the current working directory, ensuring appropriate file permissions (`0o600`).
5.  The `process.env.GEMINI_API_KEY` is updated immediately for the current session.

### Project Structure

```
.
â”œâ”€â”€ bin/
â”‚   â””â”€â”€ codesense.js                  # Main CLI executable
â”œâ”€â”€ commands/
â”‚   â”œâ”€â”€ ask.js                        # CLI 'ask' command handler
â”‚   â”œâ”€â”€ index.js                      # CLI 'index' command handler
â”‚   â”œâ”€â”€ init.js                       # CLI 'init' command handler
â”‚   â”œâ”€â”€ readme.js                     # CLI 'readme' command handler
â”‚   â”œâ”€â”€ reset.js                      # CLI 'reset' command handler
â”‚   â””â”€â”€ status.js                     # CLI 'status' command handler
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ ingest/
â”‚   â”‚   â”œâ”€â”€ chunker.js                # Chunks file content
â”‚   â”‚   â”œâ”€â”€ filter.js                 # Filters files and assigns roles
â”‚   â”‚   â”œâ”€â”€ indexState.js             # Manages index status file
â”‚   â”‚   â””â”€â”€ scanRepo.js               # Scans repository files
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ generate.js               # Interacts with Gemini LLM
â”‚   â”‚   â””â”€â”€ readmeTasks.js            # Defines README generation prompts
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ getGlobalContext.js       # Retrieves broad project context
â”‚   â”‚   â”œâ”€â”€ retrieve.js               # Retrieves relevant code chunks
â”‚   â”‚   â””â”€â”€ vector.js                 # Handles vector database operations
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ ensureEnv.js              # Checks for API key
â”‚       â””â”€â”€ getStorePaths.js          # Provides standardized storage paths
â”œâ”€â”€ package.json                      # Project metadata and dependencies
â””â”€â”€ .env                              # (Generated) Stores GEMINI_API_KEY
â””â”€â”€ .codesense/                       # (Generated) Directory for CodeSense data
    â”œâ”€â”€ vectordb/                     # (Generated) Local vector database files
    â””â”€â”€ status.json                   # (Generated) Indexing status file
```

### Environment

The only environment variable explicitly used and managed by CodeSense is:

- `GEMINI_API_KEY`: Required for authenticating with the Google Gemini API for both embedding generation and LLM interactions. It is checked by `ensureEnv.js` and configured via `codesense init`.

### ðŸ“‚ Source References

- `FILE: ask.js`
- `FILE: chunker.js`
- `FILE: codesense.js`
- `FILE: ensureenv.js`
- `FILE: filter.js`
- `FILE: generate.js`
- `FILE: getGlobalContext.js`
- `FILE: getStorePaths.js`
- `FILE: index.js`
- `FILE: indexState.js`
- `FILE: init.js`
- `FILE: package.json`
- `FILE: readme.js`
- `FILE: readmeTasks.js`
- `FILE: reset.js`
- `FILE: retrieve.js`
- `FILE: scanRepo.js`
- `FILE: status.js`
- `FILE: vector.js`

---

_Last Updated: 7/25/2024_

---

_Last Updated: 27/12/2025_
